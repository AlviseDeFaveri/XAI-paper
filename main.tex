\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote.
% If that is unneeded, please comment it out.
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{graphicx}

\usepackage{todonotes}
\newcommand{\cit}[1][]{\todo[tickmarkheight=0.2cm]{cit #1}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Explanations and cognitive bias:\\an underspoken issue of the XAI approach}

% \title{Defining Explainability in the XAI field}

\author{\IEEEauthorblockN{Alvise de' Faveri Tron}
    \IEEEauthorblockA{\textit{Politecnico di Milano} \\
        alvise.defaveri@mail.polimi.it}}

\maketitle

\begin{abstract}
    \todo[inline]{TODO}
\end{abstract}

%-----------------------------------------------------------------%
\section{Introduction}
\label{sec:intro}

AI has made giant steps since its birth in the late '50s, especially in the last decade.
Many tasks that in the past were exclusively carried out by humans, for example
in the legal, law enforcement and medical fields, are now being automated with AI. This increase in AI's capabilities is a game-changer for today's technology and society, so much that expressions like ``AI singularity'' \cit and ``Fourth industrial revolution'' \cit have been used to describe this phenomenon.


%\cit This poses a new series of challenges to our society,
% which are destined to become more relevant as this technology becomes more
% pervasive.

% by AI systems: some go as far as calling the current period a "fourth
% industrial revolution" \note{cit} caused by AI or talk about the "AI
% singularity" \note{cit} referring to the fact that nothing like this has been
% seen before in human history.

However, today's AI is far from being perfect, and our knowledge of this
technology is still partial and prone to misinterpretations.

AI tends to be unpredictable, and the nature of many AI
algorithms is such that errors in their internal models can be difficult to
identify and correct, remaining latent for potentially long periods of time. Machine Learning and Neural Networks in particular, which are central to many AI systems today, are difficult for humans to debug, and there's no such thing as ``fixing one line of code'' in these systems.

This is known as the \textit{black box problem} in AI \cit, and it poses huge ethical and practical concerns about whether we can trust this technology or not, especially given the nature of the tasks we expect them to be able to undertake in the future.

Explainable Artificial Intelligence (XAI \cit) is a recent field of AI
that aims at addressing the problem of understanding AI and making it more reliable
and trustworthy. Many interesting results have been achieved in this field, and
many more are expected to come in the next years. However, there are still some
fundamental aspects that this approach seems to struggle with, for example the problem of giving a formal definition of explainability \cit, and others that cannot be solved within the XAI framework alone, for example the problem of distinguishing correlation from causation. \cit[?]

In this paper we want to focus on the problems that arise when using an AI explanation interface as a proxy for measuring the quality of an AI algorithm, and reason about the biases that the explanation itself introduces in this measurement.

We propose a number of metrics over which an explanation can be characterized. In particular we focus on \textit{fidelity}, i.e. the adherence of the explanation to the original system, versus \textit{complexity}, which is the metric that many XAI studies use to evaluate their solutions. Our goal is to show that a tradeoff between these two metrics exists, and that this introduces the possibility for cognitive biases to play a role in evaluating an AI output.

% In particular, there seems to be a lack of uniform terminology across the
% research community when it comes to XAI. There have been attempts to define the
% notions of ``interpretability'', ``explainability'' along with
% ``reliability'',``trustworthiness'' and other similar notions, but there is no
% general consensus on how to formally define and measure these properties.
% \cit

% In this paper we want to highlight how the lack of a formal definition of ``explainability" for XAI is not just due to a lack of standardization in this newborn field, but some aspects of this problem have their roots in profound questions about intelligence, thought and cognition, which are still mainly unsolved today. We will try to propose a conceptual framework to define ``explainability'', identifying some of the problem's dimensions, in order to analyze those aspects that are not related to a specific solution. We will highlight how the problem of measuring these dimensions is more than just a technical problem, and that the subjective nature of explainability can cause cognitive biases to be accentuated in the interpretation of an AI output.

In this context, we will use  the words “explainability” and “interpretability”
interchangeably, as suggested in \cit.

The reminder of this paper is organized as follows.

Section~\ref{sec:background} provides some examples that we consider relevant to understand the XAI problem in detail.

Section~\ref{sec:xai} gives a more specific definition of XAI and a general
overview of the solutions that are being developed in this framework.

Section~\ref{sec:explainability} tries to break down the concept of explainability in several dimensions, which can be used to identify and classify AI explanations.

Finally, in Section~\ref{sec:missing} we highlight the problem of measuring fidelity and quality in an explanation, and show which cognitive biases could be introduced when examining an AI trough an explanation interface.

%-----------------------------------------------------------------%
\section{Background}
\label{sec:background}

% \subsection{Machine Learning} \label{sec:ml}

% One very popular and effective technique in AI is Machine Learning, and in
% particular Supervised Learning \cit. This approach generally consists in
% \textit{training} an algorithm by giving it as input a large number of
% instances of a given problem and letting it figure out on its own the best way
% to model it. The strength of this technique is that no previous knowledge of
% the model of the problem is needed, nor it can be enforced generally, and this
% is exactly where this technology gets an edge over more traditional computer
% science approaches.

% \todo[inline]{rimuovere?}

% \subsection{Artificial Neural Networks} \label{sec:nn}

% A particular way of implementing a Machine Learning algorithm is through
% Artificial Neural Networks (ANN), which are an extremely powerful tool that is
% being employed for many complex problems nowadays, from computer vision to
% data analysis, showing unprecedented results. The idea is to have a fixed
% structure made of interconnected \textit{neurons}, whose connections, called
% \textit{weights}, are modified in the learning phase by the learning
% algorithm.

% \todo[inline]{rimuovere?}

\subsection{AI today}
\label{sec:aiprog}

The term ``Artificial Intelligence'' has historically been used with many different meanings \cit, and this is especially true today that this subject is receiving a lot of media attention. While giving a definition of AI is out of the scope of this paper, in this Section we just want to highlight some aspects of modern AI that motivated the research for a more explainable AI.

First of all, AI algorithms are generally \textit{meta-}algorithms \cit, which means that they provide a recipe to create algorithms that can explore the solution space of a problem in an ``intelligent'' way, whatever meaning might be associated to this term. This means that the same AI algorithm can be used to solve very different problems, such as image recognition and stock market trading strategies.

Secondly, many modern AI techniques, such as Neural Networks, Genetic Algorithms, Swarm Intelligence etc., have some kind of non-determinism embedded in them: this is due to the fact that in real-world problems typically there is no optimal solution to find, but rather many suboptimal solutions that can be computed in a reasonable time, and adding randomness typically speeds up the process of finding candidate solutions by orders of magnitude.

Another aspect of many modern AIs is \textit{learning}. Machine Learning
techniques generally consist in feeding an algorithm  with a large
number of instances of a given problem and letting it figure out on its own the
best way to model it. The strength of this technique is that no previous
knowledge of the model of the problem is needed, nor it can be enforced
generally, and this is exactly where it gets an edge over more
traditional computer science approaches.

A representation of this idea is provided in Figure~\ref{fig:ml}

\begin{figure}[ht!] \centering \includegraphics[width=0.8 \linewidth]{images/ml.png}
    \caption{Difference between traditional computing an machine learning: while in the former a program is provided to the machine, in the latter it is the machine itself that is able to synthesize it.}
    \label{fig:ml} \end{figure}

% On top of this, many modern algorithms, such as Neural Networks, Ant Colony Optimization, Genetic Algorithms etc., have introduced some kind of non-determinism, which means that slightly different starting points may lead to very different solutions. These algorithms have proved to be very effective and can be applied to many problems, especially those that do not require an optimal solution, since they can explore a huge solution space until the solution is ``good enough''.

These aspects together mean that even the developer of an AI algorithm is not fully in control of all the decisions that the AI system makes, which is one of the main reasons why AI is considered to be a \textit{black box}.


\subsection{Explaining AI: a toy example}
\label{sec:example}

As an example of the problem of explaining an AI system, we can take a look to what it is like to work on a Neural Network. A concept representation of a
Neural Network's structure is depicted in Figure~\ref{fig:nn}.

\begin{figure}[ht!] \centering \includegraphics[width=0.8 \linewidth]{images/nn}
    \caption{Simple representation of an Artificial Neural Network}
    \label{fig:nn} \end{figure}

The graphical representation is useful for understanding the general
architecture, but it doesn't really tell us anything about
how the Neural Network actually works, i.e. what is the relationship between a
certain input and its output.

We could give a more precise representation of this dependency in
Figure~\ref{fig:mathnn}, which explicitly defines the mathematical relationship
between the input and the output. Without going into the details of the
mathematical formula, we can see how we would still have a hard time
understanding what a Neural Network does if we were to adopt this
representation.

\begin{figure}[ht!] \centering
    \includegraphics[width=0.9\linewidth]{images/nmodel} \caption{Mathematical
        equivalent representation} \label{fig:mathnn} \end{figure}

On the other hand, Figure~\ref{fig:dectree} represents a \textit{Decision Tree},
another family of AI algorithms.
\begin{figure}[ht!] \centering
    \includegraphics[width=0.9\linewidth]{images/dectree} \caption{A simple
        Decision Tree} \label{fig:dectree} \end{figure}

There is a clear difference with the previous example: by looking at the structure of the algorithm we can immediately be sure of what decisions it is going to take, and get a grasp on the chain of reasons that caused a specific output. The main issue of these algorithms is that, for complex problems, they tend to be outperformed by Neural Networks, and the efficient variations of these techniques such as Random Forest greatly increase the complexity of the decision trees, which can grow very big and hence be very hard to understand.

This simple example shows how the solution space to the explainability problem
has multiple dimensions, constraints and trade-offs that have to be taken into
account.

\subsection{Bias problems in AI}
\label{sec:bias}

\todo[inline]{Data bias in AI, demonstrate how vulnerable is AI to biases}

% \subsection{AI failures}
% \label{sec:aifails}

% First of all, the
% concept of complexity is different among humans and AIs: what might be simple
% for a human can be an extremely difficult task for an AI, such as image
% recognition, natural language processing and coming up with creative ideas. This
% creates a situation in which even AIs that are able to solve incredibly complex
% problems will occasionally fail in situations in which a human would have no
% problem at finding the correct solution. This happens for example in facial
% recognition, where most systems can be fooled by simply coloring the person's
% face in a certain way, or by hiding a specific portion of the face that for some
% reason happens to be important for the AI algorithm. \cit

% Another problem that has been found to affect many AI systems based on Machine
% Learning is that biased data will generate biased AI models. A famous example of
% this is the COMPAS algorithm, an algorithm used in US court systems to predict
% the likelihood that a defendant would become recidivist: due to the data that
% was used, the model predicted twice as many false positives for recidivism for
% black offenders (45\%) than white offenders (23\%). \cit Similarly, in 2015
% Amazon realized that their algorithm used for hiring employees was biased
% against women: because the algorithm was based on the number of resumes
% submitted over the past ten years, and since most of the applicants were men, it
% was trained to favor men over women. \cit The problem of debiasing AIs is still
% an open problem today, and there is no simple solution to it. \cit

% Finally, AI systems are unpredictable and obscure, even to those who develop
% them: an AI can seem to behave very well on a given test set, and yet suddenly
% break or have behave strangely when deployed in the real-world. This happens for example in ``Clever Hans'' situations, a well-known problem of AI, in which an AI might associate the outcome of a problem with an element of the input that in reality is totally unrelated from the pattern that needs to be identified, e.g. an AI system that recognizes the content of a photo based on the copyright notice at the end of it instead of looking to the real content. \cit

% \todo[inline]{flash crash}

% It is important to notice that the problems highlighted in the above examples
% cannot be associated to any particular module or feature of the AI system itself: there is
% no such thing as ``fixing one line of code'' for these systems.

%-----------------------------------------------------------------%
\section{The XAI approach}
\label{sec:xai}

\subsection{The goal}

Explainable AI is a concept that was recently formalized in a call for papers
\todo[inline, inlinewidth=1.5cm, noinlinepar]{rephrase} made by DARPA \cit, the
same agency where the term "Artificial Intelligence" was born in the first place
\cit. It is meant to be describe a new set of Artificial Intelligence systems
which are designed to be easier to understand by humans. In particular, the
goal of XAI is to make Artificial Intelligence more:

\begin{itemize}
    \item \textbf{Easy to debug} for the developers
    \item \textbf{Predictable}, so that companies and governments adopting this
          technology can be aware of the possible weaknesses of a their models,
          and can be held responsible when using bad algorithms
    \item \textbf{Trustful} for operators and end-users of this technology
\end{itemize}

Figure~\ref{fig:xai} describes the end result that is expected from XAI.


\begin{figure}[h!] \includegraphics[width=\linewidth]{images/xai.png}
    \caption{The goal of XAI as expressed in the official DARPA presentation of the XAI project. \todo[inline]{cit},} \label{fig:xai} \end{figure}

The creation of XAI requires the join effort in a variety of research fields,
from Computer Science to Cognitive Psychology, and there is still a lot of work
to do. Nevertheless already many papers have been submitted on the subject,
indicating a growing interest of the research community towards this subject.

\subsection{Current Solutions}
\label{sec:solutions}

Given the highly experimental nature of this topic, many different solutions
have been proposed by various papers in the framework of XAI, which vary greatly
in intended use, goal and adopted approach. Giannotti et al. \cit contains a classification of the existing XAI
solutions and their respective strengths and weaknesses. With no aim of being
exhaustive, here we give an abstract
classification of the main XAI solutions, based their general approach to the
problem.

XAI approaches might be classified as:

\begin{enumerate}
    \item \textbf{Visualization}: improving the understanding of a model using a
          better way to visualize its internals. One popular application of this
          approach is computer vision, where features of the learned model might
          be mapped onto the input image. \cit
    \item \textbf{Simplification}: similarly to the idea explained in
          Section~\ref{sec:example}, this approach consists in trying to adopt
          simpler models or simplify already existing models to just a set of
          important features. \cit
    \item \textbf{Approximation}: once a model has been produced by an AI, one
          explanation technique is to try and understand the dependencies
          between inputs and outputs by trying to find which output change is
          triggered by a given input change. Most of the times this means, in
          practice, creating a behavioral model of the algorithm which is
          parallel to the algorithm itself, and has no immediate correlation
          with the algorithm's internal structure.
    \item \textbf{Explanation by Example}: a nice information to have when
          trying to understand an AI model, especially in the case of
          classifiers, is an example, or a \textit{prototype}, of how the AI
          thinks that a typical member of a given class should appear. This can
          be realized in many ways, for example by attaching to a classification
          output a set of minimal changes to the input that would cause the
          output to be modified, or specify a partially filled object for each
          class.
\end{enumerate}

\todo[inline]{Immagini esemplificative}

\todo[inline]{Si può integrare con una classificazione più classica delle tecniche (internal vs external ecc)}



%-----------------------------------------------------------------%
\section{Defining Explainability}
\label{sec:explainability}

As we anticipated in Section~\ref{sec:intro}, one fundamental problem in
the field of XAI is that there is no single conventional notion of
explainability.

If, on one hand, many solutions have already been proposed to tackle the
problem, with various claims regarding their interpretability, on the other hand
the lack of a formal definition seriously challenges the findings of these
researches, casting a shade of doubt on the proposed solutions.

Mythos\cit goes as far as considering the term itself ill-defined, therefore
stating that claims about interpretability generally have a quasi-scientific
nature. Giannotti\cit on the other hand, in a review of the current state of the
art, considers the lack of a mathematical description as an obstacle for the
future development of this field. The DARPA paper \cit itself defines the
formalization of an evaluation metric for explanations as one of the goals of
the XAI project, to be developed in parallel with technical solutions.

Without discouraging the research on this matter, we want to highlight that this
is easier said than done.

\subsection{What is the scope?}

Before evaluating an explanation interface or an XAI system in general, we
should ask ourselves at least the following questions:

\

\textbf{Explainable to whom?} The concept of \textit{user} of an AI system is
not always well defined, nor is the concept of user of an explanation. This
might include:

\begin{itemize}
    \item The \textbf{developer} of the AI system, as he is only partially in
          control of what the algorithm does (refer to Section~\ref{sec:aiprog})
    \item The \textbf{operator} of an AI system: many AI algorithms nowadays are
          being used as an input for a human to make decisions on a certain
          subject
    \item The \textbf{end user} which is affected by the decision of an AI
\end{itemize}

\

\textbf{Explainable for which purpose?} Different users have different needs,
that can partially overlap, when it comes to AI explanation. More in general,
whether a certain representation can be considered explanatory depends to some
degree on what it is being used for. In the case of XAI, some common purposes
are:

\begin{itemize}
    \item \textbf{Debugging}: finding errors or underperforming portions of the
          system
    \item \textbf{Human-in-the-loop}: creating systems where human and AI
          decisions can co-exist and influence each other
    \item \textbf{Validation}: understanding if a certain model is good enough
          to be deployed for a certain tasks, where it fails and what happens
          when it fails
    \item \textbf{Appeal AI decisions}: \footnote{This last goal is not
              explicitly listed in the original scope of XAI goals, but has
              gained traction recently with the publication of \textit{right for
                  an explanation} law in EU. \todo[inline]{cit}} giving the right to
          users and citizens that are affected by AI decisions to know,
          understand and possibly appeal decisions that are automated with
          AI systems
\end{itemize}

\

It appears quite evident that different XAI solutions with different scopes and
intended users cannot be compared in the same way.

\subsection{Possible Metrics}
\label{sec:dimensions}

Bearing in mind the different goals that an XAI system can have, we can identify
a series of characteristics that are different among different solutions:

\begin{itemize}
    \item \textbf{Complexity}: how many elements are there in the explanation?
    \item \textbf{Clearness}: how cognitively hard is the explanation? How
          difficult is it to understand the correspondence between the elements
          of the explanation and the information we are trying to gain?
    \item \textbf{Informativeness}: how much information, weighted on how
          meaningful is is, can be extracted by the explanation? E.g. does the
          explanation significantly modify the level of uncertainty about the AI
          behavior?
    \item \textbf{Fidelity}: how closely does the explanation represent the
          internal functioning of the system? Are all the facts inferred from
          the explanation also applicable to the original system?
\end{itemize}

Clearly, the choice of the evaluation metric

\subsection{Can we measure all of them?}

fidelity vs complexity

\subsection{Possible biases}

cognitive bias

%-----------------------------------------------------------------%
\section{Some fundamental issues}
\label{sec:missing}


\begin{itemize}
    \item causality vs correlation
    \item previous knowledge
    \item ethical implications
\end{itemize}

%-----------------------------------------------------------------%
\section{Conclusions}
\label{sec:conclusions}

In conclusion, the main problem of XAI is that there is no single definition of
what an explanation is, it depends on the purpose and on the user of the AI
system.

For this reason, these should be considered different problems, at least the
debugging problem vs the right of explanation problem: they are not correlated
and saying that one solves the other poses some threats on the quality of the
result itself.

``I always thought something was fundamentally wrong with the universe''

\citep{adams1995hitchhiker}

\bibliographystyle{plain}
\bibliography{references}
\end{document}
