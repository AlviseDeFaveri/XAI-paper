\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote.
% If that is unneeded, please comment it out.
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[numbers]{natbib}
\usepackage{graphicx}

\usepackage{todonotes}
\newcommand{\cit}[1][]{\todo[tickmarkheight=0.2cm]{cit #1}}

\begin{document}

\title{Are we teaching machines how to lie? The potential impact of cognitive bias in AI explanation}

% \title{Trusting explanations: what can we expect from XAI?}

\author{\IEEEauthorblockN{Alvise de' Faveri Tron}
    \IEEEauthorblockA{\textit{Politecnico di Milano} \\
        alvise.defaveri@mail.polimi.it}}

\maketitle

\begin{abstract}
    Artificial Intelligence is known to suffer from interpretability problems:
    it is often hard for humans to explain why an AI-based system took a
    particular decision or to understand what is the general logic behind its
    behavior.
    This issue poses a serious ethical and technical barrier to the progress
    of AI, since the opacity of these algorithms has led, in some cases, to  systems that behave in a weird, unfair or dangerous way.
    XAI (short for eXplainable AI) is a research field
    that aims at solving this problem by providing some kind of explanation
    interface to humans that have to interact with AI systems.
    Although this has proven to be a good idea for some purposes and has generated useful tools, such as layer-per-layer visualization of the patterns that are being recognized in deep convolutional networks, there is also a potential risk associated to the creation of a complex intermediate explanation layer, since it is difficult to measure how our decisions are impacted by it. Human
    decision-making is in fact known to be affected by many cognitive bias, for instance the so-called \textit{framing effect}, which happens when our
    decisions are influenced by the way information is presented.
    If explanation interfaces are not carefully crafted they could, in principle, leverage this kind of cognitive bias to convince a human operator that the internal model is correct, or find a way of presenting it so that the predictions made by the AI are not questioned by human operators.
    % Considering that many XAI solutions focus on reducing the
    % \textit{complexity} of explanations, giving less importance to aspects
    % such as \textit{fidelity} or user understanding, which are much more difficult to measure, there is a real possibility of creating simple interfaces that sound convincing but are not completely accurate. If not counterbalanced by an comprehensive evaluation process, which is lacking today in XAI, could this create an incentive for creating explanation interfaces that are good at lying?
    % Such
    % behaviors are extremely problematic if we think about which fields are
    % starting to adopt AI as a support for human decisions - in particular the
    % legal and medical field.
    % Although many solutions have been proposed in this research framework, there is still a lack of a formal definition of \textit{interpretability}, which means that each researcher gets to choose its own metric over which the explanation quality is evaluated.
    % Many studies for example focus on the explanation \textit{complexity}, while on the other hand many argue this is not enough to define how good is an explanation. In particular, I want to focus on the fact that ignoring how faithfully an explanation represents the original model and how good it communicates it to the end user might lead to the creation of explanation interfaces that can leverage known human cognitive biases, such as the framing effect, to convince humans. In this case, there is a very thin line between being able to present information in a convincing way and lying.
\end{abstract}

% 1. why is black box a problem for AI
% ethical
% technical
% 2. AI suffers from interpretability problems
% both users and developers of this technology generally don't fully understand how it works.

% 3. XAI
% goals
% proposed solutions
% 4. no common definition of interpretability
% both because of the subjective nature of explanations
% and because of the different possible users and goals that an explanation can have
% 5. metrics


% 6. is there a tradeoff between complexity and fidelity
% yes
% Framing effect
% possible consequences
% critiques

%-----------------------------------------------------------------%
\section{Introduction}
\label{sec:intro}

AI has made giant steps since its birth in the late 1950s, especially in the last
decade. Many tasks that in the past were exclusively carried out by humans, for
example in the legal, law enforcement and medical fields, are now being
automated with AI.  \cit This increase in AI's capabilities is a game-changer for
technology and society, so much that expressions like ``AI singularity'' \cit
have been used to describe this
phenomenon.

However, today's AI is far from being perfect: many decision systems based on AI
still fail at tasks that are considered easy for humans, such as identifying
objects in images or extracting salient information from text, and our
understanding of this technology is still partial and prone to
errors.

One well known issue of modern AI-based decision systems is that they are
difficult to debug, partly because many aspects of their behavior are not in
direct control of the developer. Even when a decision system behaves well in a
set of test cases, it is difficult to understand if the internal model correctly
reflects the intended one. Machine Learning applications in particular tend to
suffer from biases \cit that are difficult to spot during the test phase, and they
tend to display an overall opaque behavior which is not easily understandable
for humans: this is known as the \textit{black box problem} in AI \cit, and it
poses huge ethical and practical concerns about whether we can trust this
technology or not, especially given the nature of the tasks we expect it to be
able to undertake in the future.

Explainable Artificial Intelligence (XAI) is a field of AI that aims at
addressing the problem of understanding AI and making it more reliable and
trustworthy. Many interesting results have been achieved in this field, and many
more are expected to come in the next years. However, there are still some
fundamental aspects that this approach seems to struggle with.

One of them is
the problem of giving a formal and comprehensive definition of interpretability, which is a quite
discussed topic in the field. \cit This lack of a common definition has led
many researchers to concentrate only on partial aspects such as the
explanation's \textit{complexity}, which is a relatively easy aspect to measure
when using proxies such as the number of elements that compose the explanation.

More difficult yet essential aspects to evaluate should be the explanation's
\textit{fidelity}, i.e. the adherence of the explanation to the original system, and its \textit{informativeness}, which is the ability to transfer to the user a faithful representation of the internal model.
But since fidelity and complexity are often in contrast when devising explanation systems, it is easy to image that the generation of a simple explanation for a complex decision can lead to omitting some aspects, that may or may not be important for evaluating it.

Taking this approach to its extreme consequences, I want to argue that a system where fidelity is never taken into account and the only metric is user feedback on the clarity of the explanation can lead to a preference for
those systems that can cover their defects better than others with respect to
the specific explanation interface in use. This can be especially harmful for ``thick'' explanation layers that try to bridge the gap between human thinking and machine learning systems, which are generally very different. If these explanation systems are able to leverage human cognitive biases, which are known to exist in this kind of situations, to sound more convincing, we could end with machines that are actually incentivezed to lie.

The final goal of this paper is to offer a reflection on how complex the
evaluation process for XAI systems is, and how dangerous are the consequences of underestimating the biases that an explanation can introduce in the decision process in which they are involved.

% In particular, there seems to be a lack of uniform terminology across the
% research community when it comes to XAI. There have been attempts to define the
% notions of ``interpretability'', ``explainability'' along with
% ``reliability'',``trustworthiness'' and other similar notions, but there is no
% general consensus on how to formally define and measure these properties.
% \cit

% In this paper we want to highlight how the lack of a formal definition of
% ``explainability" for XAI is not just due to a lack of standardization in this
% newborn field, but some aspects of this problem have their roots in profound
% questions about intelligence, thought and cognition, which are still mainly
% unsolved today. We will try to propose a conceptual framework to define
% ``explainability'', identifying some of the problem's dimensions, in order to
% analyze those aspects that are not related to a specific solution. We will
% highlight how the problem of measuring these dimensions is more than just a
% technical problem, and that the subjective nature of explainability can cause
% cognitive biases to be accentuated in the interpretation of an AI output.

% In this context, we will use  the words “explainability” and “interpretability”
% interchangeably, as suggested in \cit.

The reminder of this paper is organized as follows.

Section~\ref{sec:background} provides some background on the kind of problems that XAI tries to solve and a classification of the solutions that are currently being developed.

Section~\ref{sec:explainability} introduces the problem of defining
interpretability, and proposes a classification of the aspects that define an
explanation.

Section~\ref{sec:troubles} discusses the idea that explanation interfaces might be able to fool a human user into believing that a specific algorithm is doing the right thing.

Finally, Section~\ref{sec:conclusions} contains a list of possible critiques to
the ideas expressed in this paper and proposes possible improvements.

%-----------------------------------------------------------------%
\section{Background}
\label{sec:background}

\subsection{The need for interpretability in AI}
\label{sec:opaque}

Modern complex AI techniques, such as deep learning and genetic algorithms, are naturally opaque \cit [24], yet they constitute the foundations of today's image and speech recognition, natural language processing, autonomous driving and many other intelligent systems. These are the kind of algorithms which are being considered in this paper.

As this technology advances, we are starting to see the problems that derive from its opacity. In the 2010s public concerns about racial and other bias in the use of AI for criminal sentencing decisions and findings of creditworthiness have led to increased demand for transparent artificial intelligence. \cit [1] As a result, many academics and organizations are developing tools to help detect bias in their systems. \cit [22]

Marvin Minsky et al. raised the issue that AI can function as a form of surveillance, with the biases inherent in surveillance, suggesting HI (Humanistic Intelligence) as a way to create a more fair and balanced ``human-in-the-loop'' AI. \cit[23]

From a practical point of view, modern AI has been found to be particularly vulnerable to ``Clever Hans'' situations, in which an AI chooses the right answer for the wrong reason. A notorious example is \citet{cleverhans}, where the Fisher vector classifier trained on the PASCAL VOC 2007 data had learned to recognize horses by recognizing a copyright tag that was present in about one-fifth of the horse figures in the training dataset.

It is important to notice that the problems highlighted in the above examples
cannot be associated to any programming error of these systems, but is more of an inherent issues of these machines.

%-----------------------------------------------------------------%
% goals (introspection, human-in-the-loop, right for an explanation)
% scope (local vs global)
% techniques

\subsection{The XAI approach}
\label{sec:xai}

The term \textit{Explainable AI} refers to methods and techniques in the application of artificial intelligence that aim at improving the possibility for humans to understand its solutions. We can consider \citet{DARPA} as a starting point for modern XAI research. Figure~\ref{fig:xai} reports the goals of XAI as stated in that paper, which can be summarized as:

\begin{itemize}
    \item \textit{Accountability}: the possibility to identify precise responsibilities when things go wrong in AI.
    \item \textit{Transparency}: the possibility to understand the reasons why AIs behave in a certain way, for both debugging and trust purposes.
    \item \textit{Fairness}: the possibility to measure possible bias that has been picked up by the Ai model.
\end{itemize}

\begin{figure}[h!] \includegraphics[width=\linewidth]{images/xai.png}
    \caption{The goals of XAI as expressed in \citet{DARPA}. } \label{fig:xai} \end{figure}


The creation of such systems requires the join effort of a variety of research
fields, from Computer Science to Cognitive Psychology, and there is still a lot
of work to do. Nevertheless already many papers have been submitted on the
subject, indicating a growing interest of the research community.

\subsection{Proposed Solutions}
\label{sec:solutions}

Given the variety of goals of in this field, many different solutions
have been proposed by various papers in the framework of XAI, which vary greatly
in intended use and adopted approach. \citet{Giannotti} proposes the following high-level classification for XAI techniques, as shown in Figure~\ref{fig:xaiclass}:

\begin{itemize}
    \item \textit{Model explanation}: Global explanation of the whole model's behavior.
    \item \textit{Outcome explanation}: Local explanation of a single outcome.
    \item \textit{Model inspection}: Explanation trough introspection of the model's internals.
    \item \textit{Transparent Box design}: Making AI easier to understand by design, e.g. by using algorithms that are intrinsically clearer for humans.
\end{itemize}

\begin{figure}[h!] \includegraphics[width=\linewidth]{images/xaiclass.png}
    \caption{Taxonomy of XAI solutions provided by \citet{Giannotti}} \label{fig:xaiclass} \end{figure}

This paper focuses is in particular on those methods that are based on
reverse-engineering already existing solutions, also called \textit{post-hoc
    interpretability}, to create an explanation. This kind of systems are the ones
that are being mostly developed in the current XAI framework
(\citet{Giannotti}).

Existing solutions of this type might be classified referring to \citet{nasaxai}:

\begin{enumerate}
    \item \textit{Visualization}: these approaches focus on finding an effective
          way representation to visually represent key aspects of the AI
          behavior. One example is to use \textit{saliency masks}, such as the
          one in Figure~\ref{fig:heatmap}, to highlight which are the most
          significant portions of the input for the AI model.

          \begin{figure}[h!] \centering \includegraphics[width=0.9\linewidth]{images/dog_localization.png}
              \caption{An example of a heatmap representing the importance of pixels for a given classification decision, from blue (low importance) to red (high importance). } \label{fig:heatmap} \end{figure}

    \item \textit{Approximation}: this approach consists in using simple models for the explanation, or simplifying already existing models to just a set of important features. This is the case of \textit{single tree approximation} \cit, where the internal structure of an AI algorithm is approximated to a singe classification tree, such as in Figure~\ref{fig:dectree}.

          \begin{figure}[ht!] \centering
              \includegraphics[width=0.9\linewidth]{images/dectree} \caption{A simple Decision Tree} \label{fig:dectree} \end{figure}

    \item \textit{Causal Models (CAMEL)}: \cit the idea of this approach is to generate causal explanations of ML operation and present them to the user as intuitive narratives in an interactive, easy-to-use interface grounded in cognitive engineering theories. A scheme of the architecture needed for this approach is illustrated in Figure~\ref{fig:camel}.

          \begin{figure}[ht!] \centering
              \includegraphics[width=0.9\linewidth]{images/camel.png} \caption{Architecture of the CAMEL approach.} \label{fig:camel} \end{figure}

    \item \textit{Learning and Communicating Explainable Representations}: explanations themselves are learned as a separate part of the training process.

    \item \textit{Explanation by Example}: a nice information to have when
          trying to understand an AI model, especially in the case of
          classifiers, is an example, or a \textit{prototype}, of how the AI
          thinks that a typical member of a given class should appear and/or which characteristics should be changed to change the outcome of the classifier.
\end{enumerate}

%-----------------------------------------------------------------%
\section{Defining Explainability}
\label{sec:explainability}

As anticipated in Section~\ref{sec:intro}, one fundamental problem in
the field of XAI is that there is no single conventional notion of
explainability.
\citet{mythos} goes as far as considering the term itself ill-defined, therefore
stating that claims about interpretability generally have a quasi-scientific
nature. \citet{Giannotti} on the other hand, considers the lack of a mathematical description as an obstacle for the
future development of this field. \citet{DARPA} itself defines the
formalization of an evaluation metric for explanations as one of the goals of
the XAI project, to be developed in parallel with technical solutions.

When analyzing the problem of defining and evaluating interpretability, two questions naturally arise:

\

\textbf{Explainable to whom?} The concept of \textit{user} of an AI system is
not always well defined, nor is the concept of user of an explanation. This
might include:

\begin{itemize}
    \item The \textit{developer} of the AI system, as he is only partially in
          control of what the algorithm does
    \item The \textit{operator} of an AI system: many AI algorithms nowadays are
          being used as an input for a human to make decisions on a certain
          subject
    \item The \textit{end user} which is affected by the decision of an AI
\end{itemize}

\

\textbf{Explainable for which purpose?} Different users have different needs,
that can partially overlap, when it comes to AI explanation. More in general,
whether a certain representation can be considered explanatory depends to some
degree on what it is being used for. In the case of XAI, some common purposes
are:

\begin{itemize}
    \item \textit{Debugging}: finding errors and backtracking them to a specific reason
    \item \textit{Human-in-the-loop}: creating systems where human and AI
          decisions can co-exist and influence each other
    \item \textit{Validation}: understanding if a certain model is good enough
          to be deployed for a certain tasks, where it fails and what happens
          when it fails
    \item \textit{Appeal AI decisions}: \footnote{This goal is not
              explicitly listed in the original scope of XAI, but has
              gained traction recently with the introduction of the concept of \textit{right for an explanation} in Europe's new GDPR. (\citet{righttoexpl})} giving the right to
          users and citizens that are affected by AI decisions to know,
          understand and possibly appeal decisions that are automated with
          AI systems
\end{itemize}

\

It appears quite evident that different XAI solutions with different scopes and
intended users cannot be compared in the same way.

% If, on one hand, many solutions have already been proposed to tackle the
% problem, with various claims regarding their interpretability, on the other hand
% the lack of a formal definition seriously challenges the findings of these
% researches, casting a shade of doubt on the proposed solutions.

\subsection{Possible Metrics}
\label{sec:dimensions}

Bearing in mind the different goals that an XAI system can have, we can identify
a series of characteristics that are different among different solutions:

\begin{itemize}
    \item \textit{Complexity}: how many elements are there in the explanation?
    \item \textit{Clearness}: how cognitively hard is the explanation? How
          difficult is it to understand the correspondence between the elements
          of the explanation and the information we are trying to gain?
    \item \textit{Informativeness}: how much information, weighted on how
          meaningful is is, can be extracted by the explanation? E.g. does the
          explanation significantly modify the level of uncertainty about the AI
          behavior?
    \item \textit{Fidelity}: how closely does the explanation represent the
          internal functioning of the system? Are all the facts inferred from
          the explanation also applicable to the original system?
\end{itemize}

Clearly, the choice of the evaluation metric influences the kind of system that we are going to build.

%-----------------------------------------------------------------%
\section{The troubles of explanations}
\label{sec:troubles}

\subsection{Measuring fidelity}
\label{sec:fidelity}

\begin{enumerate}
    \item Reality
    \item Human model
    \item Machine model
    \item machine explanation
\end{enumerate}

\subsection{Decision making biases}
\label{sec:tradeoff}

\begin{enumerate}
    \item Framing effect
    \item Post-hoc interpretations
    \item Believe more to complex explanations you don't understand
\end{enumerate}

\subsection{How can an explanation leverage bias?}
\label{sec:bias}


%-----------------------------------------------------------------%
\section{Conclusions}
\label{sec:conclusions}

% critiques:
% 1. allora non si può fare niente per migliorare l'interpretabilità?
% 2. lo scenario è troppo unlikely
% 3. c'è lo stesso problema con ogni rappresentazione allora
% 4. si può fixare se valuti quanto la spiegazione incrementa la predittività
%           sì ma c'è sempre il problema del test set vs vita reale

In conclusion, the main problem of XAI is that there is no single definition of
what an explanation is, it depends on the purpose and on the user of the AI
system.

For this reason, these should be considered different problems, at least the
debugging problem vs the right of explanation problem: they are not correlated
and saying that one solves the other poses some threats on the quality of the
result itself.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
