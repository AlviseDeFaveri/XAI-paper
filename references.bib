@article{Giannotti,
  author     = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
  title      = {A Survey of Methods for Explaining Black Box Models},
  year       = {2018},
  issue_date = {January 2019},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {51},
  number     = {5},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3236009},
  doi        = {10.1145/3236009},
  abstract   = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
  journal    = {ACM Comput. Surv.},
  month      = aug,
  articleno  = {93},
  numpages   = {42},
  keywords   = {interpretability, explanations, Open the black box, transparent models}
}

@inproceedings{DARPA,
  author    = {Gunning, David},
  title     = {DARPA's Explainable Artificial Intelligence (XAI) Program},
  year      = {2017},
  isbn      = {9781450362726},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3301275.3308446},
  doi       = {10.1145/3301275.3308446},
  abstract  = {The DARPA's Explainable Artificial Intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. This talk will summarize the XAI program and present highlights from these Phase 1 evaluations.},
  booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces},
  pages     = {ii},
  numpages  = {1},
  keywords  = {explanation, machine learning, artificial intelligence, evaluation},
  location  = {Marina del Ray, California},
  series    = {IUI '19}
}

@misc{nasaxai,
  author = {Gunning, David},
  title  = {XAI for NASA},
  year   = {2018},
  url    = {https://asd.gsfc.nasa.gov/conferences/ai/program/003-XAIforNASA.pdf}
}

@article{righttoexpl,
  author  = {Selbst, Andrew D and Powles, Julia},
  title   = {{Meaningful information and the right to explanation}},
  journal = {International Data Privacy Law},
  volume  = {7},
  number  = {4},
  pages   = {233-242},
  year    = {2017},
  month   = {12},
  issn    = {2044-3994},
  doi     = {10.1093/idpl/ipx022},
  url     = {https://doi.org/10.1093/idpl/ipx022},
  eprint  = {https://academic.oup.com/idpl/article-pdf/7/4/233/22923065/ipx022.pdf}
}

@article{cleverhans,
  title    = {Unmasking {Clever} {Hans} predictors and assessing what machines really learn},
  volume   = {10},
  issn     = {2041-1723},
  url      = {http://www.nature.com/articles/s41467-019-08987-4},
  doi      = {10.1038/s41467-019-08987-4},
  language = {en},
  number   = {1},
  urldate  = {2020-12-15},
  journal  = {Nature Communications},
  author   = {Lapuschkin, Sebastian and Wäldchen, Stephan and Binder, Alexander and Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
  month    = dec,
  year     = {2019},
  pages    = {1096}
}

@article{mythos,
  author  = {Lipton, Zachary},
  year    = {2016},
  month   = {10},
  pages   = {},
  title   = {The Mythos of Model Interpretability},
  volume  = {61},
  journal = {Communications of the ACM},
  doi     = {10.1145/3233231}
}
@inproceedings{framingeffect,
  author = {Kim, Taenyun and Song, Hayeon},
  year   = {2020},
  month  = {04},
  pages  = {},
  title  = {The Effect of Message Framing and Timing on the Acceptance of Artificial Intelligence's Suggestion},
  doi    = {10.1145/3334480.3383038}
}

@inproceedings{explainingexpl,
  author    = {L. H. {Gilpin} and D. {Bau} and B. Z. {Yuan} and A. {Bajwa} and M. {Specter} and L. {Kagal}},
  booktitle = {2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)},
  title     = {Explaining Explanations: An Overview of Interpretability of Machine Learning},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {80-89},
  doi       = {10.1109/DSAA.2018.00018}
}


@book{norvig,
  author    = {Russell, Stuart and Norvig, Peter},
  title     = {Artificial Intelligence: A Modern Approach},
  year      = {2009},
  isbn      = {0136042597},
  publisher = {Prentice Hall Press},
  address   = {USA},
  edition   = {3rd}
}

@inproceedings{minsky,
  author    = {M. {Minsky} and R. {Kurzweil} and S. {Mann}},
  booktitle = {2013 IEEE International Symposium on Technology and Society (ISTAS): Social Implications of Wearable Computing and Augmediated Reality in Everyday Life},
  title     = {The society of intelligent veillance},
  year      = {2013},
  volume    = {},
  number    = {},
  pages     = {13-17},
  doi       = {10.1109/ISTAS.2013.6613095}
}

@article{tufekci2018youtube,
  title   = {YouTube, the great radicalizer},
  author  = {Tufekci, Zeynep},
  journal = {The New York Times},
  volume  = {10},
  pages   = {2018},
  year    = {2018}
}

@article{solsman,
  title   = {YouTube's AI is the puppet master over most of what you watch},
  author  = {Solsman, Joan},
  journal = {cnet},
  year    = {2018}
}

@article{epstein,
  author  = {Epstein, Robert and Robertson, Ronald E.},
  year    = {2015},
  month   = {08},
  pages   = {},
  title   = {The search engine manipulation effect (SEME) and its possible impact on the outcomes of elections},
  volume  = {112},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  doi     = {10.1073/pnas.1419828112}
}

@inproceedings{buolamwini2018gender,
  title     = {Gender shades: Intersectional accuracy disparities in commercial gender classification},
  author    = {Buolamwini, Joy and Gebru, Timnit},
  booktitle = {Conference on fairness, accountability and transparency},
  pages     = {77--91},
  year      = {2018}
}

@article{hiring,
  author  = {Jeffrey Dastin},
  year    = {2018},
  month   = {10},
  pages   = {},
  title   = {Amazon scraps secret AI recruiting tool that showed bias against women},
  journal = {Reuters},
  url     = {https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G}
}

@article{compas,
  title   = {Machine Bias},
  author  = {Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner},
  journal = {ProPublica},
  year    = {2018}
}